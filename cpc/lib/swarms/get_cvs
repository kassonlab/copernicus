#!/usr/bin/env python
#
# Get Collective Variables
# 
# Take the resulting confs (.gro's) from the swarm iteration step (number of swarms * number of string points)
# and pre-process into a form that the reparametrize step can use.
#
# For dihedral swarms, this amounts to using the Gromacs tool g_rama to extract the phi/psi dihedrals from the
# .gro's into .xvg's.
#
# For position-restraint swarms, we use trjconv to fit the possibly drifted swarm results into the reference structure
# in the top-level provided tpr, and output only the Protein data into new .gro's which reparametrize can read directly.
# Note that the total protein-drift from the swarm step is probably very marginal, but the specified initial and end
# structures are supposed to stay locked. 
# Can this pose a problem if the energy gradient DOES in fact want to shift/rotate the protein as a whole? I guess this
# can't be a part of the string optimization since it would mean bad equilibration of the protein membrane embedding
# or similar.

# Notes:
# https://docs.python.org/2/library/threading.html#thread-objects
# http://stackoverflow.com/questions/11954021/how-to-run-parallel-programs-in-python

# This file is part of Copernicus
# http://www.copernicus-computing.org/
# 
# Copyright (C) 2011-2015, Sander Pronk, Iman Pouya, Bjorn Wesen, Erik Lindahl and others.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License version 2 as published 
# by the Free Software Foundation
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.


import sys
import os
import math
import subprocess
import os.path
try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO

import cpc.dataflow
from cpc.dataflow import StringValue
from cpc.dataflow import FloatValue
from cpc.dataflow import IntValue
from cpc.dataflow import RecordValue
from cpc.dataflow import ArrayValue
from cpc.dataflow import FileValue
from cpc.lib.gromacs import cmds

class FEError(cpc.dataflow.ApplicationError):
    pass


def run(inp, out):
    cmdnames = cmds.GromacsCommands()
    pers = cpc.dataflow.Persistence(os.path.join(inp.getPersistentDir(),
                                                 "persistent.dat"))

    # TODO: make sure this works with installs with only g_rama_mpi 

    # Run once
    getcvs = pers.get('getcvs')

    if getcvs is None or getcvs < 1:

        # Figure out if we are going to use position or dihedral restraints for the swarm controls
        if inp.getInput('use_posres') is not None and inp.getInput('use_posres') > 0:
            use_posres = 1
        else:
            use_posres = 0

        # The confs[] input array is complete-flagged so when we get here, it should be filled in

        confs = inp.getInput('confs')
        tpr = inp.getInput('tpr')

        if use_posres == 1:
            # CVs are protein atom positions
            # Cluster and fit the incoming system and output the Protein index only to save space
            # Run all trjconv's in parallel using Popen and the shell

            # Since we can't cluster and fit at the same time, and we can't use named pipes or bash process
            # substitution due to the fact that trjconv tries to autodetect the output type by looking at the output
            # filename, we have to run two separate sets of trjconvs, the first pass doing clustering of all systems
            # and the second pass doing the fitting

            FNULL = open(os.devnull, 'w') # sink for output spam

            # Run the points sequentially but all sub-confs (nbr swarms) in parallel, which is more easy on the computer
            # than all out parallel which creates like 500 threads that can go OOM if you're unlucky
            for i in range(len(confs)):

                # Cluster pass
                trjprocs_clustering = []
                clustering_intermediaries = []
                subconfs = inp.getInput('confs[%d]' % i)
                for j in range(len(subconfs)):
                    conf = inp.getInput('confs[%d][%d]' % (i, j))
                    # First choice trjconv asks for is what to cluster (=> Protein), second choice is what to output (=> System)
                    # We need to output the whole system here as that is what the tpr file specifies, and the same tpr is used
                    # for the second trjconv pass that gets the output from here as input
                    # Remember the temporary file used so we can remove them easily when we're done
                    clustering_intermediaries.append('0%03d_0%03d_clustered.gro' % (i, j))
                    # Will this actually work?  Set up with a pipe instead?
                    trjprocs_clustering.append(subprocess.Popen(['echo -e "Protein\nSystem\n" | %s -f %s -s %s -o %s -pbc cluster'
                                                                 % (cmdnames.trjconv, conf, tpr, clustering_intermediaries[-1])],
                                                                stdout=FNULL, stderr=FNULL, shell=True))

                # Wait for completion of the clustering trjconvs
                for j in range(len(trjprocs_clustering)):
                    trjprocs_clustering[j].communicate()

                # Fitting pass, and set the output names (does not matter that they are not ready yet, the setOut only writes
                # a setting command to this script's XML output which is parsed after the script as a whole is done)

                trjprocs_fitting = []
                for j in range(len(subconfs)):
                    outfile = '0%03d_0%03d.gro' % (i, j)
                    # First choice trjconv asks for is what to fit against (=> Backbone), second choice is what to output (=> Protein)
                    # Set pbc none so we don't destroy the clustering we just did
                    trjprocs_fitting.append(subprocess.Popen(['echo -e "Backbone\nProtein\n" | %s -f %s -s %s -o %s -pbc none -fit rot+trans'
                                                              % (cmdnames.trjconv, clustering_intermediaries[j], tpr, outfile)],
                                                             stdout=FNULL, stderr=FNULL, shell=True))
                    cv_i_j = os.path.join(inp.getOutputDir(), outfile)
                    out.setOut('cvs[%d][%d]' % (i, j), FileValue(cv_i_j))

                # Wait for completion of the fitting trjconvs
                for j in range(len(trjprocs_fitting)):
                    trjprocs_fitting[j].communicate()
                    # Can now also remove the associated intermediary
                    os.remove(clustering_intermediaries[j])

        else:
            # CVs are dihedral restraints, produce xvg files describing the phi/psi angles for the selected residues
            # Start all g_rama's in parallel
            ramaprocs = []
            FNULL = open(os.devnull, 'w') # sink for output spam
            for i in range(len(confs)):
                subconfs = inp.getInput('confs[%d]' % i)
                for j in range(len(subconfs)):
                    conf = inp.getInput('confs[%d][%d]' % (i, j))
                    cmd = cmdnames.rama.split() + ['-f', conf, '-s', tpr, '-o',
                                                   '0%03d_0%03d.xvg' % (i, j)]
                    ramaprocs.append(subprocess.Popen(cmd,
                                                      stdout=FNULL, stderr=FNULL))

            # Wait for each process to complete, and set the output file correctly
            q = 0
            for i in range(len(confs)):
                subconfs = inp.getInput('confs[%d]' % i)
                for j in range(len(subconfs)):
                    ramaprocs[q].communicate()
                    q += 1
                    cv_i_j = os.path.join(inp.getOutputDir(), '0%03d_0%03d.xvg' % (i, j))
                    out.setOut('cvs[%d][%d]' % (i, j), FileValue(cv_i_j))

            FNULL.close()

        getcvs = 1

    getcvs = pers.set('getcvs', getcvs)

    pers.write()

# read the input data
inf = StringIO()
inf.write(sys.stdin.read())
inf.seek(0)
sys.stderr.write("\n-----> Starting\n")
inf.seek(0)
inp = cpc.dataflow.readInput(inf)

if inp.testing():
    # TODO: make it possible for sub-functions to be checked now.
    cpc.util.plugin.testCommand("g_bar -version")
    cpc.util.plugin.testCommand("grompp -version")
    cpc.util.plugin.testCommand("trjconv -version")
    cpc.util.plugin.testCommand("gmxdump -version")
    # try to import msmproject and thereby msmbuilder
    sys.exit(0)


# prepare the output data
out = inp.getFunctionOutput()

run(inp, out)

out.writeXML(sys.stdout)
#sys.stderr.write('\n')
#out.writeXML(sys.stderr)
#sys.stderr.write('\n')
sys.stderr.write("-----> Finished.\n")


